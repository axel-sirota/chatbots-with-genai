{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip wheel setuptools"
      ],
      "metadata": {
        "id": "SRik1VTXjBjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio flask ninja torch accelerate transformers"
      ],
      "metadata": {
        "id": "YQ1yVmKhjBOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVx2RMFYRnky"
      },
      "outputs": [],
      "source": [
        "!MAX_JOBS=12 python -m pip -v install flash-attn --no-build-isolation  --use-pep517\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75lpa3EwR0-l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, logging\n",
        "from transformers import set_seed\n",
        "from peft import PeftModel\n",
        "\n",
        "# Set a fixed random seed for reproducibility.\n",
        "set_seed(0)\n",
        "logging.set_verbosity_error()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------- Sentiment Analysis ----------------\n",
        "# Load a sentiment-analysis pipeline using DistilBERT.\n",
        "sentiment_pipeline = None\n",
        "\n",
        "# ---------------- Model Loading ----------------\n",
        "# Load the Phi-3.5-mini-instruct model and tokenizer.\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# Load the model (trust_remote_code must be True for this model).\n",
        "model = None\n",
        "\n",
        "tokenizer = None\n",
        "\n",
        "# Create the text-generation pipeline using our model.\n",
        "# (This pipeline expects a list of message dictionaries as input.)\n",
        "chatbot = None\n",
        "\n",
        "# ---------------- Response Function ----------------\n",
        "def respond(message: str,\n",
        "            history: list,\n",
        "            system_message: str,\n",
        "            max_tokens: int,\n",
        "            temperature: float,\n",
        "            top_p: float):\n",
        "    \"\"\"\n",
        "    This function builds a conversation history (a list of dicts)\n",
        "    and calls the Phi-3.5-mini-instruct pipeline with that history.\n",
        "\n",
        "    It first ensures that the history starts with the provided system message.\n",
        "    It then runs sentiment analysis on the new user message.\n",
        "    If the sentiment is strongly negative, the user message is prefixed with \"Angry:\".\n",
        "    The updated history is then passed to the pipeline.\n",
        "\n",
        "    Generation parameters (max_tokens, temperature, top_p) are passed along.\n",
        "\n",
        "    Troubleshooting suggestions:\n",
        "      - If the responses seem hallucinated or off-topic, try adjusting temperature (try higher for more creative, lower for deterministic) or top_p.\n",
        "      - You can print the history inside this function to verify the conversation structure.\n",
        "      - Ensure that the input history is a list of dictionaries with \"role\" and \"content\" keys.\n",
        "    \"\"\"\n",
        "    # If no history exists, initialize with the system message.\n",
        "    if history is None or len(history) == 0:\n",
        "        history = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    # Run sentiment analysis on the user message.\n",
        "    sentiment = sentiment_pipeline(message)[0]\n",
        "    if sentiment[\"label\"] == \"NEGATIVE\" and sentiment[\"score\"] > 0.85:\n",
        "        user_entry = {\"role\": \"user\", \"content\": \"Angry: \" + message}\n",
        "    else:\n",
        "        user_entry = {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "    history.append(user_entry)\n",
        "\n",
        "    generation_args = {None}\n",
        "\n",
        "    # Pass the full conversation history (a list of dicts) directly to the pipeline.\n",
        "    output = None\n",
        "    assistant_reply = None\n",
        "\n",
        "    history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "    return assistant_reply\n",
        "\n",
        "# ---------------- Gradio Chat Interface ----------------\n",
        "# This interface uses additional inputs for system message and generation parameters.\n",
        "demo = gr.ChatInterface(\n",
        "    fn=respond,\n",
        "    type=\"messages\",  # Conversation history is a list of message dictionaries.\n",
        "    title=\"Phi-3.5-mini Chatbot\",\n",
        "    description=(\n",
        "        \"A chatbot powered by microsoft/Phi-3.5-mini-instruct. \"\n",
        "        \"It uses sentiment analysis to tag angry messages and accepts conversation history as a list of dicts. \"\n",
        "        \"Adjust parameters below to test and troubleshoot responses.\"\n",
        "    ),\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(value=\"You are a helpful AI assistant.\", label=\"System message\"),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=500, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.0, maximum=4.0, value=1.0, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p (nucleus sampling)\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXWxzn3GU0vu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}