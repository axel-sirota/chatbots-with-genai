{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU-TGXO3tE9k"
      },
      "outputs": [],
      "source": [
        "!pip install requests tqdm faiss-cpu 'transformers==4.44.2' 'accelerate==0.33.0' torch sentence-transformers textblob gensim 'numpy<2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_Zx-cM6tKky"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Directory to store downloaded and extracted data\n",
        "DATA_DIR = Path(\"./mimic_textbooks\")\n",
        "\n",
        "# Download and extract the dataset zip file\n",
        "def download_and_extract_zip(url, extract_to=DATA_DIR):\n",
        "    # Ensure the directory exists\n",
        "    extract_to.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download the zip file\n",
        "    zip_path = extract_to / \"textbooks.zip\"\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(zip_path, \"wb\") as file:\n",
        "        for chunk in tqdm(response.iter_content(chunk_size=1024), unit='KB'):\n",
        "            if chunk:\n",
        "                file.write(chunk)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"Dataset downloaded and extracted.\")\n",
        "\n",
        "# Download and extract textbooks\n",
        "dataset_url = \"https://www.dropbox.com/scl/fi/gk1y8ll3d7wllwbb24kqe/textbooks.zip?rlkey=cdpqf8cbeu3difouvhwsc866w&st=resv96io&dl=1\"\n",
        "download_and_extract_zip(dataset_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./mimic_textbooks/en"
      ],
      "metadata": {
        "id": "aiMCdb61NjLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e9bbpzvtNFX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from gensim.utils import simple_preprocess\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load text files\n",
        "def load_text_files(directory):\n",
        "    texts = []\n",
        "    # read the file text over the whole directory and append to texts\n",
        "    for file_path in Path(directory).glob(\"*.txt\"):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            texts.append(file.read())\n",
        "    return texts\n",
        "\n",
        "# Cleaning and preprocessing function\n",
        "def clean_and_tokenize(text):\n",
        "    # Tokenize with gensim\n",
        "    pass\n",
        "\n",
        "# Chunk text into fixed-size chunks\n",
        "def chunk_text(text, chunk_size=1000):\n",
        "    pass\n",
        "\n",
        "# Load, clean, correct, and chunk documents\n",
        "documents = load_text_files(DATA_DIR / \"en\")\n",
        "cleaned_documents = [pass]\n",
        "chunked_documents = []\n",
        "None\n",
        "\n",
        "print(f\"Total document chunks created: {len(chunked_documents)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7qLJ9NYtUnV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = None\n",
        "model = None\n",
        "\n",
        "# Function to generate embeddings for all chunks in a batch\n",
        "def get_embeddings_in_batch(texts, batch_size=16):\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Wrap the loop with tqdm to display a progress bar\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "        # Tokenize the batch of texts\n",
        "        inputs = None\n",
        "        # Generate embeddings on the GPU\n",
        "        outputs = None  # [batch_size, sequence_length, hidden_size], get ;ast state\n",
        "        batch_embeddings = None  # [batch_size, hidden_size]\n",
        "\n",
        "        # Append batch embeddings to the list\n",
        "        all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "    return np.array(all_embeddings)\n",
        "\n",
        "# Generate embeddings for all document chunks in batches\n",
        "embeddings = get_embeddings_in_batch(chunked_documents, batch_size=64)\n",
        "print(f\"Generated embeddings for {len(embeddings)} document chunks.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFJHoP_0tX5F"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Define the dimension of embeddings\n",
        "dimension = 384  # Embedding size from MiniLM model\n",
        "index = None\n",
        "\n",
        "# Convert embeddings to NumPy array for FAISS\n",
        "embedding_matrix = None\n",
        "\n",
        "# Add embeddings to FAISS index\n",
        "None\n",
        "print(f\"Total embeddings indexed: {index.ntotal}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb7gGdMetZX_"
      },
      "outputs": [],
      "source": [
        "# Example query for testing\n",
        "\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "    outputs = model(**inputs).last_hidden_state\n",
        "    return torch.mean(outputs, dim=1).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "\n",
        "query_text = \"What are causes of heart failure?\"\n",
        "query_embedding = get_embedding(query_text)\n",
        "query_embedding = np.array(query_embedding).reshape(1, -1).astype('float32')\n",
        "\n",
        "# Search FAISS for the most similar documents\n",
        "k = 5  # Number of closest documents to retrieve\n",
        "distances, indices = None\n",
        "\n",
        "# Retrieve and print the most similar chunks\n",
        "print(\"Top similar document chunks:\")\n",
        "for idx in indices[0]:\n",
        "    print(chunked_documents[idx])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}