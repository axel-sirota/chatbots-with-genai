{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVx2RMFYRnky"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers gradio flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75lpa3EwR0-l"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoConfig,\n",
        "    pipeline,\n",
        "    logging\n",
        ")\n",
        "\n",
        "# Optional: reduce logging verbosity.\n",
        "logging.set_verbosity_error()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------- Ethical Filtering ----------------\n",
        "banned_words = [\"hate\", \"kill\", \"stupid\", \"idiot\", \"terrorist\", \"bomb\", \"nazi\", \"racist\", \"sexist\"]\n",
        "\n",
        "def contains_banned_words(text: str) -> bool:\n",
        "    text = text or \"\"\n",
        "    text_lower = text.lower()\n",
        "    for word in banned_words:\n",
        "        if re.search(r\"\\b\" + re.escape(word) + r\"\\b\", text_lower):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# ---------------- Sentiment Analysis ----------------\n",
        "sentiment_pipeline = None  # Load sentiment model\n",
        "\n",
        "\n",
        "# ---------------- Model Loading ----------------\n",
        "base_checkpoint = \"google/flan-t5-large\"\n",
        "\n",
        "# Load the base model and tokenizer.\n",
        "tokenizer = None\n",
        "model = None\n",
        "\n",
        "# ---------------- Response Function ----------------\n",
        "def respond(message: str,\n",
        "            history: list,  # history is managed automatically by Gradio.\n",
        "            system_message: str,\n",
        "            max_tokens: int,\n",
        "            temperature: float,\n",
        "            top_p: float):\n",
        "    \"\"\"\n",
        "    Respond to the user's message.\n",
        "    - Applies ethical filtering and sentiment checks.\n",
        "    - Builds a prompt using the system message and current user message.\n",
        "    - Generates a response using our LoRA-finetuned FLAN-T5-large model.\n",
        "    Returns a single string reply.\n",
        "    \"\"\"\n",
        "    if message is None or message.strip() == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    # Ethical filtering.\n",
        "    None\n",
        "\n",
        "    # Sentiment analysis.\n",
        "    sentiment = sentiment_pipeline(message)[0]\n",
        "    if sentiment[\"label\"] == \"NEGATIVE\" and sentiment[\"score\"] > 0.90:\n",
        "        return \"I sense some negativity in your message. Let's try to keep our conversation respectful.\"\n",
        "\n",
        "    # Build prompt: use the system message plus the current user message.\n",
        "    prompt = None # Build Prompt\n",
        "    inputs = None # Tokenize\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Ensure sampling parameters are in effect.\n",
        "    outputs = None # Generate Answer\n",
        "    response = None # Decode\n",
        "    # Remove the prompt from the output if it's there.\n",
        "    if response.startswith(prompt):\n",
        "        response = response[len(prompt):].strip()\n",
        "    return response\n",
        "\n",
        "# ---------------- Gradio Chat Interface ----------------\n",
        "demo = gr.ChatInterface(\n",
        "    fn=respond,\n",
        "    type=\"messages\",  # This indicates that the conversation history is maintained by Gradio.\n",
        "    title=\"Ethical Chatbot\",\n",
        "    description=\"Chatbot using a FLAN-T5-large model for responses and DistilBERT for sentiment analysis with ethical filtering.\",\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(value=\"You are a friendly Chatbot. Please respond helpfully.\", label=\"System message\"),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=150, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"Top-p (nucleus sampling)\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXWxzn3GU0vu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}