{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip wheel"
      ],
      "metadata": {
        "id": "ml489Z7ZVKBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU-TGXO3tE9k"
      },
      "outputs": [],
      "source": [
        "!pip install requests tqdm faiss-cpu torch sentence-transformers textblob gensim numba ninja 'numpy<2' 'transformers==4.44.2' 'accelerate==0.33.0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!MAX_JOBS=12 python -m pip -v install flash-attn --no-build-isolation  --use-pep517"
      ],
      "metadata": {
        "id": "9Cl3tK3hWDKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_Zx-cM6tKky"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "from gensim.utils import simple_preprocess\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Directory to store downloaded and extracted data\n",
        "DATA_DIR = Path(\"./mimic_textbooks\")\n",
        "# URLs for the dataset and pre-chunked JSON\n",
        "dataset_url = \"https://www.dropbox.com/scl/fi/gk1y8ll3d7wllwbb24kqe/textbooks.zip?rlkey=cdpqf8cbeu3difouvhwsc866w&st=resv96io&dl=1\"\n",
        "CHUNKED_DOCUMENTS_PATH = Path(\"./chunked_documents.json\")\n",
        "CHUNKED_DOCUMENTS_URL = \"https://www.dropbox.com/scl/fi/07wd0zwvz2xcq80hy5f91/chunked_documents.json?rlkey=jwvfpczo4zeyke9j74cdphovi&st=oeqmcfi8&dl=1\"\n",
        "\n",
        "# Download and extract the dataset zip file\n",
        "def download_and_extract_zip(url, extract_to=DATA_DIR):\n",
        "    # Ensure the directory exists\n",
        "    extract_to.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download the zip file\n",
        "    zip_path = extract_to / \"textbooks.zip\"\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(zip_path, \"wb\") as file:\n",
        "        for chunk in tqdm(response.iter_content(chunk_size=1024), unit='KB'):\n",
        "            if chunk:\n",
        "                file.write(chunk)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"Dataset downloaded and extracted.\")\n",
        "\n",
        "# Load text files from a given directory\n",
        "def load_text_files(directory):\n",
        "    texts = []\n",
        "    for file_path in Path(directory).glob(\"*.txt\"):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            texts.append(file.read())\n",
        "    return texts\n",
        "\n",
        "# Cleaning and preprocessing function\n",
        "def clean_and_tokenize(text):\n",
        "    # Remove extra spaces, lowercase text, and remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    tokens = simple_preprocess(text)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Chunk text into fixed-size chunks\n",
        "def chunk_text(text, chunk_size=1000):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# Main process:\n",
        "if CHUNKED_DOCUMENTS_PATH.exists():\n",
        "    print(\"Loading existing chunked_documents.json...\")\n",
        "    with open(CHUNKED_DOCUMENTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        chunked_documents = json.load(f)\n",
        "else:\n",
        "    print(\"chunked_documents.json does not exist. Trying to download from remote URL...\")\n",
        "    try:\n",
        "        response = requests.get(CHUNKED_DOCUMENTS_URL, allow_redirects=True)\n",
        "        with open(CHUNKED_DOCUMENTS_PATH, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Successfully downloaded chunked_documents.json from remote URL.\")\n",
        "        with open(CHUNKED_DOCUMENTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "            chunked_documents = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download chunked_documents.json: {e}\")\n",
        "        print(\"Creating chunked_documents.json from dataset...\")\n",
        "\n",
        "        # Download and extract the textbooks if needed\n",
        "        download_and_extract_zip(dataset_url)\n",
        "\n",
        "        # Load, clean, and process documents\n",
        "        documents = load_text_files(DATA_DIR / \"en\")\n",
        "        cleaned_documents = [clean_and_tokenize(doc) for doc in documents]\n",
        "        chunked_documents = []\n",
        "        for doc in cleaned_documents:\n",
        "            chunked_documents.extend(chunk_text(doc))\n",
        "        print(f\"Total document chunks created: {len(chunked_documents)}\")\n",
        "\n",
        "        # Save the chunked documents to JSON\n",
        "        with open(CHUNKED_DOCUMENTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(chunked_documents, f)\n",
        "        print(\"chunked_documents.json created.\")\n",
        "\n",
        "print(f\"Total document chunks available: {len(chunked_documents)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFJHoP_0tX5F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Assuming 'chunked_documents' is already defined from earlier processing\n",
        "INDEX_PATH = \"./faiss_index.idx\"\n",
        "FAISS_INDEX_URL = \"https://www.dropbox.com/scl/fi/05ez2886nz5fkkcqsv6hs/faiss_index.idx?rlkey=yil6ollju5smk04upluenqot4&st=yu0oji49&dl=1\"\n",
        "dimension = 384  # Embedding size from MiniLM model\n",
        "# Load model and tokenizer (using PyTorch) only when needed\n",
        "retrieval_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "retrieval_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    print(\"Loading existing FAISS index from disk...\")\n",
        "    index = faiss.read_index(INDEX_PATH)\n",
        "    print(f\"Total embeddings indexed: {index.ntotal}\")\n",
        "else:\n",
        "    print(\"FAISS index does not exist. Trying to download from remote URL...\")\n",
        "    try:\n",
        "        response = requests.get(FAISS_INDEX_URL, allow_redirects=True)\n",
        "        with open(INDEX_PATH, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Successfully downloaded FAISS index from remote URL.\")\n",
        "        index = faiss.read_index(INDEX_PATH)\n",
        "        print(f\"Total embeddings indexed: {index.ntotal}\")\n",
        "    except Exception as e:\n",
        "        print(\"FAISS index not found. Creating index...\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        retrieval_model.to(device)\n",
        "        retrieval_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "        # Function to generate embeddings for all chunks in a batch\n",
        "        def get_embeddings_in_batch(texts, batch_size=16):\n",
        "            all_embeddings = []\n",
        "\n",
        "            # Wrap the loop with tqdm to display a progress bar\n",
        "            for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "                # Tokenize the batch of texts\n",
        "                inputs = retrieval_tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "                # Generate embeddings on the GPU\n",
        "                outputs = retrieval_model(**inputs).last_hidden_state  # [batch_size, sequence_length, hidden_size]\n",
        "                batch_embeddings = torch.mean(outputs, dim=1).cpu().detach().numpy()  # [batch_size, hidden_size]\n",
        "\n",
        "                # Append batch embeddings to the list\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "            return np.array(all_embeddings)\n",
        "\n",
        "        # Generate embeddings for all document chunks in batches\n",
        "        embeddings = get_embeddings_in_batch(chunked_documents, batch_size=128)\n",
        "        print(f\"Generated embeddings for {len(embeddings)} document chunks.\")\n",
        "\n",
        "        # Create the FAISS index and add embeddings\n",
        "        index = faiss.IndexFlatL2(dimension)\n",
        "        # Ensure embeddings are in the correct shape and type\n",
        "        embedding_matrix = np.array([embedding.flatten() for embedding in embeddings]).astype('float32')\n",
        "        index.add(embedding_matrix)\n",
        "        print(f\"Total embeddings indexed: {index.ntotal}\")\n",
        "\n",
        "        # Write the FAISS index to disk\n",
        "        faiss.write_index(index, INDEX_PATH)\n",
        "        print(f\"FAISS index written to {INDEX_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrival Method"
      ],
      "metadata": {
        "id": "2OPM94lHkuG_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb7gGdMetZX_"
      },
      "outputs": [],
      "source": [
        "retrieval_model.cpu().eval()\n",
        "\n",
        "# Function to generate embeddings for a new query\n",
        "def get_query_embedding(query):\n",
        "    pass\n",
        "\n",
        "# Load FAISS index with existing embeddings\n",
        "embedding_dim = 384\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "# Function to retrieve relevant documents based on the query\n",
        "def retrieve_documents(query, top_k=5):\n",
        "    pass\n",
        "\n",
        "# Test retrieval component\n",
        "sample_query = \"What are the symptoms of heart failure?\"\n",
        "similar_documents = retrieve_documents(sample_query)\n",
        "print(\"Retrieved documents:\", similar_documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Method"
      ],
      "metadata": {
        "id": "ylpKt7T1r8zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generation_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\", trust_remote_code=True)\n",
        "generation_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "generation_model.to(device).eval()\n"
      ],
      "metadata": {
        "id": "YeBUoKYNxQ51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to generate a response using retrieved context\n",
        "def generate_response(query, context, max_new_tokens=100):\n",
        "    input_text = None\n",
        "\n",
        "    # Tokenize the input and move tensors to GPU\n",
        "    inputs = None\n",
        "\n",
        "    # Generate response using max_new_tokens to control output length\n",
        "    None\n",
        "\n",
        "    # Decode the generated response\n",
        "    response_text = generation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "# Testing generation with retrieved documents as context\n",
        "retrieved_text = \" \".join(similar_documents)  # Concatenate retrieved documents as context\n",
        "response = generate_response(sample_query, retrieved_text)\n",
        "print(\"Generated response:\", response)"
      ],
      "metadata": {
        "id": "6abG8NMVr6hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2LBrTCo_mX6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}