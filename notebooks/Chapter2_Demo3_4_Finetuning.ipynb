{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNlDV7O1InfjkWP1eQCMbR7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"453654ea71c94a65a1be92ce2bc972f6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6db8ce6913c9401f92d9a2d28d3a427e","IPY_MODEL_f84897a6a9b14085a04a0002ec8eb4ee","IPY_MODEL_21a4d4720ba441d99116d4cc972bf701"],"layout":"IPY_MODEL_d3d89035de364c6aad873eb79e04aed2"}},"6db8ce6913c9401f92d9a2d28d3a427e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2025c9c266664742bd912549a300f0bf","placeholder":"​","style":"IPY_MODEL_30e53ac30bfc47aeae70af637aec0397","value":"Map: 100%"}},"f84897a6a9b14085a04a0002ec8eb4ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96d6a385ef2d4de9bd898de89d48a684","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6c4512d705745e2ba15270d4033ebce","value":1000}},"21a4d4720ba441d99116d4cc972bf701":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7916891ef9aa472cba29dde58d772d03","placeholder":"​","style":"IPY_MODEL_ad41cb592a384632a5960f196b15699d","value":" 1000/1000 [00:00&lt;00:00, 1152.51 examples/s]"}},"d3d89035de364c6aad873eb79e04aed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2025c9c266664742bd912549a300f0bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30e53ac30bfc47aeae70af637aec0397":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96d6a385ef2d4de9bd898de89d48a684":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6c4512d705745e2ba15270d4033ebce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7916891ef9aa472cba29dde58d772d03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad41cb592a384632a5960f196b15699d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xi8Ajvj8aWL","executionInfo":{"status":"ok","timestamp":1740017496544,"user_tz":180,"elapsed":2542,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"8b9cafda-03da-4797-fd06-8ce1fb4b73c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["!pip install requests transformers datasets torch peft flask"]},{"cell_type":"code","source":["import torch\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer\n",")\n","from datasets import load_dataset\n","from peft import get_peft_model, LoraConfig, TaskType\n","\n","# 1. Load the translation dataset (\"opus_books\", \"en-fr\")\n","dataset = load_dataset(\"opus_books\", \"en-fr\")\n","split_datasets = dataset[\"train\"].select(range(10000)).train_test_split(test_size=0.1, seed=42)\n","train_dataset = split_datasets[\"train\"]\n","eval_dataset = split_datasets[\"test\"]\n"],"metadata":{"id":"f2VEo7Zh8iEB","executionInfo":{"status":"ok","timestamp":1740017498231,"user_tz":180,"elapsed":1686,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\n","# 2. Load the tokenizer from the FLAN-T5-large checkpoint\n","model_checkpoint = \"google/flan-t5-large\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","# 3. Define the preprocessing function that now also computes decoder inputs and attention masks.\n","prefix = \"translate English to French: \"\n","\n","# Custom implementation of shift_tokens_right\n","def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n","    \"\"\"\n","    Shift input ids one token to the right, placing the decoder_start_token_id at the beginning.\n","    Replace any -100 values with pad_token_id.\n","    \"\"\"\n","    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n","    shifted_input_ids[:, 0] = decoder_start_token_id\n","    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n","    # Replace -100 values with pad_token_id\n","    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n","    return shifted_input_ids\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + ex[\"en\"] for ex in examples[\"translation\"]]\n","    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n","    # Use the text_target argument (replacing the deprecated as_target_tokenizer)\n","    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n","    labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=\"max_length\")\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    # Compute decoder_input_ids individually (since lengths vary).\n","    decoder_input_ids = []\n","    for seq in labels[\"input_ids\"]:\n","        seq_tensor = torch.tensor(seq)\n","        # Unsqueeze to add batch dimension\n","        shifted_seq_tensor = shift_tokens_right(\n","            seq_tensor.unsqueeze(0),\n","            pad_token_id=tokenizer.pad_token_id,\n","            decoder_start_token_id=tokenizer.pad_token_id\n","        )\n","        decoder_input_ids.append(shifted_seq_tensor.squeeze(0).tolist())\n","    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n","\n","    # Compute decoder_attention_mask for each sequence.\n","    decoder_attention_mask = []\n","    for seq in decoder_input_ids:\n","        mask = [1 if token != tokenizer.pad_token_id else 0 for token in seq]\n","        decoder_attention_mask.append(mask)\n","    model_inputs[\"decoder_attention_mask\"] = decoder_attention_mask\n","    return model_inputs\n","\n","# Map the preprocessing function over the datasets (batched mode)\n","train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n","eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=eval_dataset.column_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["453654ea71c94a65a1be92ce2bc972f6","6db8ce6913c9401f92d9a2d28d3a427e","f84897a6a9b14085a04a0002ec8eb4ee","21a4d4720ba441d99116d4cc972bf701","d3d89035de364c6aad873eb79e04aed2","2025c9c266664742bd912549a300f0bf","30e53ac30bfc47aeae70af637aec0397","96d6a385ef2d4de9bd898de89d48a684","d6c4512d705745e2ba15270d4033ebce","7916891ef9aa472cba29dde58d772d03","ad41cb592a384632a5960f196b15699d"]},"id":"JHKrf1XU9TmW","executionInfo":{"status":"ok","timestamp":1740017499371,"user_tz":180,"elapsed":1138,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"d771b43c-c5ff-4820-f57d-0eb0877df48d"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"453654ea71c94a65a1be92ce2bc972f6"}},"metadata":{}}]},{"cell_type":"code","source":["\n","# 4. Load the FLAN-T5-large model using AutoModel and wrap it with PEFT LoRA\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","lora_config = LoraConfig(\n","    task_type=TaskType.SEQ_2_SEQ_LM,\n","    r=4,             # LoRA rank.\n","    lora_alpha=32,   # Scaling factor.\n","    lora_dropout=0.1,\n","    target_modules=[\"lm_head\"]  # Only apply LoRA to the final lm_head.\n",")\n","model = get_peft_model(model, lora_config)\n","print(\"Model wrapped with LoRA configuration.\")\n","\n","# 5. Prepare the data collator (for dynamic padding)\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZE2xJEu9YT3","executionInfo":{"status":"ok","timestamp":1740017500413,"user_tz":180,"elapsed":1040,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"50750814-33f9-400b-f6c7-cae6e7c576cc"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model wrapped with LoRA configuration.\n"]}]},{"cell_type":"code","source":["from transformers import TrainerCallback\n","# Custom callback to print trainable parameters at training start.\n","class PrintTrainableParamsCallback(TrainerCallback):\n","    def on_train_begin(self, args, state, control, **kwargs):\n","        model = kwargs.get(\"model\")\n","        total_params = sum(p.numel() for p in model.parameters())\n","        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","        print(f\"Total parameters: {total_params}\")\n","        print(f\"Trainable parameters: {trainable_params}\")\n","        return control\n"],"metadata":{"id":"AOytP_Os9vul","executionInfo":{"status":"ok","timestamp":1740017500419,"user_tz":180,"elapsed":5,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# 6. Set up training arguments\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./flan_t5_finetuned_opus_books\",\n","    eval_strategy=\"epoch\",  # Replaces evaluation_strategy.\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=16,   # Small batch size for demo purposes.\n","    per_device_eval_batch_size=2,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    save_total_limit=2,\n","    predict_with_generate=True,\n","    fp16=torch.cuda.is_available(),\n","    report_to=[]  # Disable logging integrations (e.g., wandb).\n",")\n","\n","# 7. Initialize the Trainer with the custom callback and using processing_class instead of tokenizer.\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    processing_class=tokenizer,  # Use processing_class instead of the deprecated tokenizer argument.\n","    data_collator=data_collator,\n","    callbacks=[PrintTrainableParamsCallback()]\n",")\n","\n","\n","# 8. Fine-tune the model\n","print(\"Starting fine-tuning...\")\n","# trainer.train()\n","print(\"Fine-tuning complete.\")\n","\n","# Optionally save the fine-tuned model (including LoRA adapters)\n","model.save_pretrained(\"./flan_t5_finetuned_lora\")\n","tokenizer.save_pretrained(\"./flan_t5_finetuned_lora\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2CZKikv9fDC","executionInfo":{"status":"ok","timestamp":1740020814741,"user_tz":180,"elapsed":372,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"2a077e88-f5ec-46a6-de70-fc0204e2d58c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting fine-tuning...\n","Fine-tuning complete.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n","  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"]},{"output_type":"execute_result","data":{"text/plain":["('./flan_t5_finetuned_lora/tokenizer_config.json',\n"," './flan_t5_finetuned_lora/special_tokens_map.json',\n"," './flan_t5_finetuned_lora/spiece.model',\n"," './flan_t5_finetuned_lora/added_tokens.json',\n"," './flan_t5_finetuned_lora/tokenizer.json')"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["\n","# 9. Define a helper function for translation using the fine-tuned model\n","def translate_text(text):\n","    prompt = prefix + text\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n","    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return translation.replace(prefix, \"\").strip()\n","\n","# 10. Chatbot loop: send translation prompts and get responses\n","if __name__ == \"__main__\":\n","    print(\"\\nTranslation Chatbot (type 'exit' to quit)\")\n","    while True:\n","        user_input = input(\"You: \").strip()\n","        if user_input.lower() == \"exit\":\n","            break\n","        translation = translate_text(user_input)\n","        print(\"Bot (French Translation):\", translation)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xvf452xB9gan","executionInfo":{"status":"ok","timestamp":1740017820929,"user_tz":180,"elapsed":199778,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"6a2d9687-5b61-450f-c9dd-0dde975faf6d"},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Translation Chatbot (type 'exit' to quit)\n","You: exit\n"]}]},{"cell_type":"code","source":["# prompt: make a zip file out of the files flan_t5_finetuned_opus_books_lora and download to local computer\n","\n","!zip -r flan_t5_finetuned_opus_books_lora.zip flan_t5_finetuned_opus_books_lora\n","!ls\n","\n","#To download the file, you can use the following code in a code cell:\n","#from google.colab import files\n","#files.download('flan_t5_finetuned_opus_books_lora.zip')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TM4B6XcJWlV","executionInfo":{"status":"ok","timestamp":1740017890990,"user_tz":180,"elapsed":7526,"user":{"displayName":"Axel Sirota","userId":"02089179879199828401"}},"outputId":"ba49aca4-9496-4143-a160-734aadb4f7c5"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: flan_t5_finetuned_opus_books_lora/ (stored 0%)\n","  adding: flan_t5_finetuned_opus_books_lora/tokenizer_config.json (deflated 95%)\n","  adding: flan_t5_finetuned_opus_books_lora/README.md (deflated 66%)\n","  adding: flan_t5_finetuned_opus_books_lora/tokenizer.json (deflated 74%)\n","  adding: flan_t5_finetuned_opus_books_lora/adapter_model.safetensors (deflated 7%)\n","  adding: flan_t5_finetuned_opus_books_lora/adapter_config.json (deflated 54%)\n","  adding: flan_t5_finetuned_opus_books_lora/special_tokens_map.json (deflated 85%)\n","  adding: flan_t5_finetuned_opus_books_lora/spiece.model (deflated 48%)\n","flan_t5_finetuned_opus_books\t   flan_t5_finetuned_opus_books_lora.zip\n","flan_t5_finetuned_opus_books_lora  sample_data\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9x2orZXPJdgd"},"execution_count":null,"outputs":[]}]}